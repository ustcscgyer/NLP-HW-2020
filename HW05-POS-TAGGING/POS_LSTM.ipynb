{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "### Due March 17th, 23:59\n",
    "\n",
    "In this homework you will be implementing a LSTM model for POS tagging.\n",
    "\n",
    "You are given the following files:\n",
    "- `POS_NEMM.ipynb`: Notebook file for NEMM model (Optional)\n",
    "- `POS_LTML.ipynb`: Notebook file for MTML model\n",
    "- `train.txt`: Training set to train your model\n",
    "- `test.txt`: Test set to report your modelâ€™s performance\n",
    "- `tags.csv`: Treebank tag universe\n",
    "- `sample_prediction.csv`: Sample file your prediction result should look like\n",
    "- `utils/`: folder containing all utility code for the series of homeworks\n",
    "\n",
    "\n",
    "### Deliverables (zip them all)\n",
    "\n",
    "- pdf or html version of your final notebook\n",
    "- Use the best model you trained, generate the prediction for test.txt, name the\n",
    "output file prediction.csv (Be careful: the best model in your training set might not\n",
    "be the best model for the test set).\n",
    "- writeup.pdf: summarize the method you used and report their performance.\n",
    "If you worked on the optional task, add the discussion. Add a short essay\n",
    "discussing the biggest challenges you encounter during this assignment and\n",
    "what you have learnt.\n",
    "\n",
    "(**You are encouraged to add the writeup doc into your notebook\n",
    "using markdown/html langauge, just like how this notes is prepared**)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import sparse\n",
    "\n",
    "# add utils folder to path\n",
    "p = os.path.dirname(os.getcwd())\n",
    "if p not in sys.path:\n",
    "    sys.path = [p] + sys.path\n",
    "    \n",
    "from utils.hw5 import load_data, save_prediction, ignore_class_accuracy, whole_sentence_accuracy\n",
    "from utils.general import show_keras_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tags` is a dictionary that maps the [Treebank tag](https://www.clips.uantwerpen.be/pages/mbsp-tags) to its numerical encoding. There are 45 tags in total, plus a special tag `START (tags[-1])` to indicate the beginning of a sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T05:10:05.453780Z",
     "start_time": "2019-04-03T05:10:04.295178Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tags = list(pd.read_csv('tags.csv', index_col=0).tag_encode.keys())\n",
    "\n",
    "train, train_label = load_data(\"train.txt\")\n",
    "train, dev, train_label, dev_label = train_test_split(train, train_label)\n",
    "test, _ = load_data(\"test.txt\")\n",
    "\n",
    "print(\"Training set: %d\" % len(train))\n",
    "print(\"Dev set: %d\" % len(dev))\n",
    "print(\"Testing set: %d\" % len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T05:14:20.197334Z",
     "start_time": "2019-04-03T05:14:20.146236Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "class POS_LSTMM:\n",
    "    \"\"\"\n",
    "    To help you focus on the LSTM model, I have made most part of the code ready, make sure you\n",
    "    read all the parts to understand how the code works. You only need to modify the prepare method \n",
    "    to add the RNN model.\n",
    "    \"\"\"\n",
    "    def __init__(self, tag_vocab=tags, max_sent_len=40, \n",
    "                 voc_min_freq=5, **kwargs):\n",
    "        \"\"\"\n",
    "        input: \n",
    "            tag_vocab: tag dictionary, you will less likely need to change this\n",
    "            voc_min_freq: use this to truncate low frequency vocabulary\n",
    "            max_sent_len: truncate/pad all sentences to this length\n",
    "            \n",
    "            kwargs: Use as needed to pass extra parameters\n",
    "        \"\"\"\n",
    "        self.vocab = []\n",
    "        self.reverse_vocab = {}\n",
    "        self.tag_vocab = tag_vocab\n",
    "        self.reverse_tag_vocab = {k:v for v, k in enumerate(tag_vocab)}\n",
    "        self._voc_min_freq = voc_min_freq\n",
    "        self._max_sent_len = max_sent_len\n",
    "\n",
    "        \"\"\"\n",
    "        Feel free to add code here as you need\n",
    "        \"\"\"\n",
    "\n",
    "    def collect_vocab(self, X):\n",
    "        \"\"\"\n",
    "        Create vocabulary from all input data\n",
    "        input:\n",
    "            X: list of sentences\n",
    "        \"\"\"\n",
    "        vocab = Counter([t for s in X for t in s])\n",
    "        vocab = {k: v for k, v in vocab.items() if v > self._voc_min_freq}\n",
    "        vocab = [\"<PAD>\", \"<UNK>\"] + sorted(vocab, key=lambda x: vocab[x], reverse=True)\n",
    "        reverse_vocab = {k: v for v, k in enumerate(vocab)}\n",
    "        \n",
    "        return vocab, reverse_vocab\n",
    "                \n",
    "    def transform_X(self, X):\n",
    "        \"\"\"\n",
    "        Translate input raw data X into trainable numerical data\n",
    "        input:\n",
    "            X: list of sentences\n",
    "        \"\"\"\n",
    "        X_out = []\n",
    "        \n",
    "        default = self.reverse_vocab['<UNK>']\n",
    "        for sent in X:\n",
    "            X_out.append([self.reverse_vocab.get(t, default) for t in sent])\n",
    "            \n",
    "        X_out = pad_sequences(sequences=X_out, maxlen=self._max_sent_len, \n",
    "                              padding='post', truncating='post',\n",
    "                              value=self.reverse_vocab['<PAD>'])\n",
    "        \n",
    "        return X_out\n",
    "    \n",
    "    def transform_Y(self, Y):\n",
    "        \"\"\"\n",
    "        Translate input raw data Y into trainable numerical data\n",
    "        input:\n",
    "            y: list of list of tags\n",
    "        \"\"\"\n",
    "        Y_out = [] \n",
    "        \n",
    "        for labs in Y:\n",
    "            Y_out.append([self.reverse_tag_vocab[lab] for lab in labs])\n",
    "            \n",
    "        Y_out = pad_sequences(sequences=Y_out, maxlen=self._max_sent_len, \n",
    "                              padding='post', truncating='post',\n",
    "                              value=self.reverse_tag_vocab['<PAD>'])\n",
    "        \n",
    "        return Y_out\n",
    "    \n",
    "    def prepare(self, X, Y):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            X: list of sentences\n",
    "            y: list of list of tags\n",
    "        \"\"\"\n",
    "        self.vocab, self.reverse_vocab = self.collect_vocab(X)\n",
    "        X, Y = self.transform_X(X), self.transform_Y(Y)\n",
    "        \n",
    "        model = Sequential()\n",
    "        \"\"\"\n",
    "        Write your own model here\n",
    "        Hints:\n",
    "            - Rember to use embedding layer at the beginning\n",
    "            - Use Bidrectional LSTM to take advantage of both direction history  \n",
    "        \"\"\"\n",
    "        raise NotImplemented(\"Implement Me!!\")\n",
    "\n",
    "        \"\"\"\n",
    "        You can read the source code to understand how ignore_class_accuracy works.\n",
    "        The reason of using this customized metric is because we have padded the training \n",
    "        data with lots of '<PAD>' tag. It's easy and useless to predict this tag, we need \n",
    "        to ignore this tag when calculate the accuracy.\n",
    "        \"\"\"\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=Adam(0.001),\n",
    "                      metrics=['accuracy', \n",
    "                               ignore_class_accuracy(self.reverse_tag_vocab['<PAD>']),\n",
    "                               whole_sentence_accuracy(self.reverse_tag_vocab['<PAD>'])])\n",
    "\n",
    "        self.model = model\n",
    "        \n",
    "        return self\n",
    "        \n",
    "        \n",
    "    def fit(self, X, Y, batch_size=128, epochs=10):\n",
    "        X, Y = self.transform_X(X), self.transform_Y(Y)\n",
    "        self.model.fit(X, to_categorical(Y, num_classes=len(self.tag_vocab)),\n",
    "                       batch_size=batch_size, \n",
    "                       epochs=epochs, validation_split=0.2)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        results = []\n",
    "        X_new = self.transform_X(X)\n",
    "        Y_pred = self.model.predict_classes(X_new)\n",
    "    \n",
    "        for i, y in enumerate(Y_pred):\n",
    "            results.append(\n",
    "                [self.tag_vocab[y[j]] for j in range(min(len(X[i]), len(X_new[i])))]\n",
    "            )\n",
    "            \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lstm = POS_LSTMM().prepare(train, train_label)\n",
    "lstm.model.summary()\n",
    "show_keras_model(lstm.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = POS_LSTMM().prepare(train, train_label)\n",
    "lstm.fit(train, train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T05:15:01.277528Z",
     "start_time": "2019-04-03T05:14:59.503776Z"
    }
   },
   "outputs": [],
   "source": [
    "prediction = lstm.predict(test)\n",
    "save_prediction(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
