{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "### Due March 17th, 23:59\n",
    "\n",
    "In this homework you will be implementing a LSTM model for POS tagging.\n",
    "\n",
    "You are given the following files:\n",
    "- `POS_NEMM.ipynb`: Notebook file for NEMM model (Optional)\n",
    "- `POS_LTML.ipynb`: Notebook file for MTML model\n",
    "- `train.txt`: Training set to train your model\n",
    "- `test.txt`: Test set to report your modelâ€™s performance\n",
    "- `tags.csv`: Treebank tag universe\n",
    "- `sample_prediction.csv`: Sample file your prediction result should look like\n",
    "- `utils/`: folder containing all utility code for the series of homeworks\n",
    "\n",
    "\n",
    "### Deliverables (zip them all)\n",
    "\n",
    "- pdf or html version of your final notebook\n",
    "- Use the best model you trained, generate the prediction for test.txt, name the\n",
    "output file prediction.csv (Be careful: the best model in your training set might not\n",
    "be the best model for the test set).\n",
    "- writeup.pdf: summarize the method you used and report their performance.\n",
    "If you worked on the optional task, add the discussion. Add a short essay\n",
    "discussing the biggest challenges you encounter during this assignment and\n",
    "what you have learnt.\n",
    "\n",
    "(**You are encouraged to add the writeup doc into your notebook\n",
    "using markdown/html langauge, just like how this notes is prepared**)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T05:16:56.542579Z",
     "start_time": "2019-04-03T05:16:56.495628Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import sparse\n",
    "import pickle\n",
    "\n",
    "# add utils folder to path\n",
    "p = os.path.dirname(os.getcwd())\n",
    "if p not in sys.path:\n",
    "    sys.path = [p] + sys.path\n",
    "    \n",
    "from utils.hw5 import load_data, save_prediction, check_path_search\n",
    "from utils.general import show_keras_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tags` is a dictionary that maps the [Treebank tag](https://www.clips.uantwerpen.be/pages/mbsp-tags) to its numerical encoding. There are 45 tags in total, plus a special tag `START (tags[-1])` to indicate the beginning of a sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T05:10:05.453780Z",
     "start_time": "2019-04-03T05:10:04.295178Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tags = pd.read_csv('tags.csv', index_col=0).tag_encode.to_dict()\n",
    "\n",
    "train, train_label = load_data(\"train.txt\")\n",
    "train, dev, train_label, dev_label = train_test_split(train, train_label)\n",
    "test, _ = load_data(\"test.txt\")\n",
    "\n",
    "print(\"Training set: %d\" % len(train))\n",
    "print(\"Dev set: %d\" % len(dev))\n",
    "print(\"Testing set: %d\" % len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shortest Path Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the class we introduced the first order markov model (HMM and MEMM). In these models, the probability distribution of the current state at `t` is conditioned on the previous state at `t-1`. \n",
    "\n",
    "Suppose:\n",
    "- `L` is the number of tags\n",
    "- `T` is the length of the sentence\n",
    "- $g_t$ is the predicted tag at time t \n",
    "\n",
    "The probability of the current stage can be discribed by a `(L+1)*L` matrix (taking into account of the extra `START` tag). The model output for a given sentence should be a `T*(L+1)*L` matrix (`Ms`) where:\n",
    "\\begin{equation*}\n",
    "    Ms[t, i, j] = p(g_t==j|g_{t-1}==i)\n",
    "\\end{equation*}   \n",
    "\n",
    "However, we need a post processing to convert the model output into the final prediction. The idea is to find such a path $\\{g_0, g_1, g_2, ..., g_T\\}$ so that the path probability is maximized:\n",
    "\n",
    "\\begin{equation*}\n",
    "    P(g_0, g_1, ..., g_T) = \\Pi_{t=1}^n p(g_t|g_{t-1})\n",
    "\\end{equation*}\n",
    "\n",
    "In practice, you may find that $\\Pi_{t=1}^n p(g_t|g_{t-1})$ diminish to zero very quickly. For numerical stability, we maximize the logarithm of the path probability:\n",
    "\\begin{equation*}\n",
    "    lg(P(g_0, g_1, ..., g_T)) = \\sum_{t=1}^n lg(p(g_t|g_{t-1}))\n",
    "\\end{equation*}\n",
    "\n",
    "## Greedy method\n",
    "Heuristically one can use greedy path search, meaning to greedily choose the step that maximize the current probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T04:12:13.509676Z",
     "start_time": "2019-04-03T04:12:13.478794Z"
    }
   },
   "outputs": [],
   "source": [
    "def greedy_path_search(Ms):\n",
    "    \"\"\" \n",
    "    greedy path search to get the label prediction path\n",
    "    \n",
    "    input: np.array(T, L+1, L)\n",
    "    return: list, prediction path\n",
    "    \"\"\"\n",
    "    path = [Ms.shape[1] - 1]\n",
    "    for t, M in enumerate(Ms):\n",
    "        gt = np.argmax(M[path[-1]])\n",
    "        path.append(gt)\n",
    "    return path[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " here we load some test data, and see what is the output of this `greedy_path_search`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T04:12:14.955414Z",
     "start_time": "2019-04-03T04:12:14.751960Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"examples.pickle\",\"rb\") as f:\n",
    "    examples = pickle.load(f)\n",
    "    \n",
    "print(\"example 0: \", greedy_path_search(examples[0]['Ms']))\n",
    "print(\"example 1: \", greedy_path_search(examples[1]['Ms']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viterbi path search \n",
    "\n",
    "There is no guarrantee that the greedy search can always find the optimal solution. You can run the test below and see that for most of the time, the greedy search leads to the wrong result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T04:12:32.499494Z",
     "start_time": "2019-04-03T04:12:32.435593Z"
    }
   },
   "outputs": [],
   "source": [
    "check_path_search(greedy_path_search, examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce the [viterbi algorithm](https://en.wikipedia.org/wiki/Viterbi_algorithm). Instead of choosing the maximum probability at the current step, we use DP (dynamical programming) to memorize the best paths to all the possible tags at time `t` and the corresponding probability. This algorithm has been explained in details in class. Refer to your notes if you are in doubts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T03:47:48.855459Z",
     "start_time": "2019-04-03T03:47:48.821757Z"
    }
   },
   "outputs": [],
   "source": [
    "def viterbi_path_search(Ms):\n",
    "    \"\"\"\n",
    "        :param\n",
    "        Ms: np.array(T, L+1, L)\n",
    "        return: list, prediction path\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "        Write your code below\n",
    "\n",
    "        Hints:\n",
    "        1. You need a matrix to store the current path probabilities\n",
    "        2. You need a matrix to store the backpointers to retrieve \n",
    "        the path\n",
    "        3. Remember that the path probability diminish to zero fast, what\n",
    "        should you do for numerical stability?\n",
    "    \"\"\"\n",
    "    \n",
    "    raise NotImplemented(\"Implement Me!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T07:02:39.245845Z",
     "start_time": "2019-04-02T07:02:37.351095Z"
    }
   },
   "outputs": [],
   "source": [
    "check_path_search(viterbi_path_search, examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MEMM (1st order) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training clas we use here is a little similar to the one we have in HW01. I provide the code for you so you can focus on the important part of the model, `featurization`. First go through the code and understand how it works. It helps you to accomplish the work after."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T05:14:20.197334Z",
     "start_time": "2019-04-03T05:14:20.146236Z"
    }
   },
   "outputs": [],
   "source": [
    "class BaseFeaturizer:\n",
    "    \"\"\"\n",
    "    A template for the Featurizer. Later you will have to create your own featurizer, it has to follow\n",
    "    the class signature as `BaseFeaturizer`. \n",
    "    In the training class, this will be used like this:\n",
    "    \n",
    "    featurize = BaseFeaturizer()\n",
    "    features = featurize(t, sent, prev_tag)\n",
    "    \n",
    "    see below for the meaning of parameters\n",
    "    \"\"\"\n",
    "    def __call__(self, t, sent, prev_tag, **kwargs):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            t: int, the current index of the word in the sentence\n",
    "            sent: list[string], the entire current sentence\n",
    "            prev_tag: string, previous word's tag. If t=0, prev_tag would be `START`\n",
    "                (remember in First order MEMM, the entire sentence both before and after the current\n",
    "                word, together with the previous 1 tage can be used to construct features. )\n",
    "            kwargs: Use this as needed to pass extra parameters\n",
    "        \"\"\"\n",
    "        features = dict()\n",
    "        features['PREV_TAG_%s' % prev_tag] = 1\n",
    "        features['UNIGRAM_%s' % sent[t]] = 1\n",
    "\n",
    "        return features\n",
    "\n",
    "    \n",
    "class FOMEMM:\n",
    "    \"\"\"\n",
    "    First order maximum entropy Markov model\n",
    "    \"\"\"\n",
    "    def __init__(self, Featurizer=BaseFeaturizer, path_search=viterbi_path_search, \n",
    "                 tag_vocab=tags, **kwargs):\n",
    "        \"\"\"\n",
    "        input: \n",
    "            Featurizer: BaseFeaturizer class\n",
    "            path_search: function for path search\n",
    "            tag_vocab: tag dictionary, you will less likely need to change this\n",
    "            \n",
    "            kwargs: Use as needed to pass extra parameters\n",
    "        \"\"\"\n",
    "        self.feature_vocab = {}\n",
    "        self.featurize = Featurizer()\n",
    "        self.path_search = path_search\n",
    "        self.tag_vocab = tag_vocab\n",
    "        self.reverse_tag_vocab = {v:k for k, v in tag_vocab.items()}\n",
    "        self.model = None\n",
    "\n",
    "        \"\"\"\n",
    "        Feel free to add code here as you need\n",
    "        \"\"\"\n",
    "\n",
    "    def collect_features(self, X, y):\n",
    "        \"\"\"\n",
    "        Create feature vocabulary from all input data\n",
    "        input:\n",
    "            X: list of sentences\n",
    "            y: list of list of tags\n",
    "        \"\"\"\n",
    "        feature_counts = defaultdict(int)\n",
    "        for sent, labs in zip(X, y):\n",
    "            for t in range(len(labs)):\n",
    "                if t == 0: \n",
    "                    prev_tag = \"START\"\n",
    "                else:\n",
    "                    prev_tag = labs[t-1]\n",
    "                feats = self.featurize(t, sent, prev_tag)\n",
    "                \n",
    "                for f in feats:\n",
    "                    feature_counts[f] += 1\n",
    "                    \n",
    "        \"\"\"\n",
    "        Below I just use all the features I collect to construct\n",
    "        the feature_vocab. Feel free to add code here as needed.\n",
    "        \"\"\"\n",
    "        \n",
    "        feature_vocab = {k:i+1 for i, k in enumerate(feature_counts.keys())}\n",
    "        feature_vocab['_UNKNOWN_'] = 0\n",
    "        \n",
    "        return feature_vocab\n",
    "                \n",
    "    def pipeline(self, X, y):\n",
    "        \"\"\"\n",
    "        Translate all input raw data into trainable numerical data\n",
    "        input:\n",
    "            X: list of sentences\n",
    "            y: list of list of tags\n",
    "        \"\"\"\n",
    "        ntot = sum([len(sent) for sent in X])\n",
    "        X_new = sparse.dok_matrix((ntot, len(self.feature_vocab)))\n",
    "        \n",
    "        i = 0\n",
    "        for sent, labs in zip(X, y):\n",
    "            for t in range(len(labs)):\n",
    "                if t == 0: \n",
    "                    prev_tag = \"START\"\n",
    "                else:\n",
    "                    prev_tag = labs[t-1]\n",
    "                feats = self.featurize(t, sent, prev_tag)\n",
    "                \n",
    "                for f, v in feats.items():\n",
    "                    X_new[i, self.feature_vocab[f]] = v\n",
    "                    \n",
    "                i += 1\n",
    "                  \n",
    "        y_new = np.array([self.tag_vocab[t] for labs in y for t in labs])\n",
    "        \n",
    "        return X_new, y_new\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            X: list of sentences\n",
    "            y: list of list of tags\n",
    "        \"\"\"\n",
    "        self.feature_vocab = self.collect_features(X, y)\n",
    "        self.X, self.y = self.pipeline(X, y)\n",
    "        \n",
    "        self.model = LogisticRegression(C=1.0, multi_class='auto')\n",
    "        self.model.fit(self.X, self.y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        results = []\n",
    "        L = len(self.tag_vocab) - 1\n",
    "        for j, sent in enumerate(X):\n",
    "            T = len(sent)\n",
    "            Y_pred = np.zeros((T, L+1, L))\n",
    "            for t in range(len(sent)):\n",
    "                # We need to make one prediction for each possible prev_tag\n",
    "                for prev_tag, i in self.tag_vocab.items():\n",
    "                    feats = self.featurize(t, sent, prev_tag)\n",
    "                    X0 = sparse.lil_matrix((1, len(self.feature_vocab)))\n",
    "                    for f, v in feats.items():\n",
    "                        X0[0, self.feature_vocab.get(f, 0)] = v\n",
    "                        \n",
    "                    prob = self.model.predict_proba(X0)\n",
    "                    Y_pred[t, i, :] = prob\n",
    "            \n",
    "            tag_path = self.path_search(Y_pred)\n",
    "            results.append([self.reverse_tag_vocab[t] for t in tag_path])\n",
    "            \n",
    "            print(\"Generation predictions, %.3f%% complete.\" % (j*100/len(X)), end=\"\\r\")\n",
    "            \n",
    "        return results\n",
    "\n",
    "    def score(self, X, y):\n",
    "        tot_tokens = sum([len(sent) for sent in X])\n",
    "        tot_sents = len(X)\n",
    "        \n",
    "        y_pred = self.predict(X)\n",
    "        \n",
    "        right_tokens, right_sents = 0, 0\n",
    "        \n",
    "        for labs0, labs1 in zip(y, y_pred):\n",
    "            right_sents += (labs0 == labs1)\n",
    "            for lab0, lab1 in zip(labs0, labs1):\n",
    "                right_tokens += (lab0 == lab1)\n",
    "                \n",
    "        return {\"tokens\" : 1.0 * right_tokens / tot_tokens, \n",
    "                \"sentences\": 1.0 * right_sents / tot_sents}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check how the model works with the base featurizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T05:20:50.935744Z",
     "start_time": "2019-04-03T05:17:46.029658Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fomemm = FOMEMM().fit(train, train_label)\n",
    "fomemm.score(dev[:10], dev_label[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement your own featurizer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T04:39:10.058296Z",
     "start_time": "2019-04-03T04:39:10.026168Z"
    }
   },
   "outputs": [],
   "source": [
    "class BetterFeaturizer(BaseFeaturizer):\n",
    "    \"\"\"\n",
    "    A template for the Featurizer. Later you will have to create your own featurizer, it has to follow\n",
    "    the class signature as `BaseFeaturizer`. \n",
    "    In the training class, this will be used like this:\n",
    "    \n",
    "    featurize = BaseFeaturizer()\n",
    "    features = featurize(t, sent, prev_tag)\n",
    "    \n",
    "    see below for the meaning of parameters\n",
    "    \"\"\"\n",
    "    def __call__(self, t, sent, prev_tag, **kwargs):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            t: int, the current index of the word in the sentence\n",
    "            sent: list[string], the entire current sentence\n",
    "            prev_tag: string, previous word's tag. If t=0, prev_tag would be `START`\n",
    "                (remember in First order MEMM, the entire sentence both before and after the current\n",
    "                word, together with the previous 1 tage can be used to construct features. )\n",
    "            kwargs: Use this as needed to pass extra parameters\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        Implement your code here\n",
    "        \"\"\"\n",
    "        raise NotImplemented(\"Implement Me!!\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fomemm = FOMEMM(Featurizer=BetterFeaturizer).fit(train, train_label)\n",
    "fomemm.score(dev, dev_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T05:15:01.277528Z",
     "start_time": "2019-04-03T05:14:59.503776Z"
    }
   },
   "outputs": [],
   "source": [
    "prediction = fomemm.predict(test)\n",
    "save_prediction(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
