{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> <font color=\"red\"> STAY SAFE !! </font> </center></h1>\n",
    "\n",
    "\n",
    "<h1><center>HW4 Language Modeling (LM)</center></h1>\n",
    "<h2><center>Due: April 2020 10th, 23:59</center></h2>\n",
    "\n",
    "In this homework, you will first implement a simple bigram language model on a dataset containing news headlines, learn basic concepts of marcov modeling, words sampling, and perplexity. \n",
    "\n",
    "Then things start get very fun and open ended. You will be shown a simple word based RNN LM. Understand how it works, and then apply changes to it as you wish. Things you can try but not limited to:\n",
    "\n",
    "1. Word based RNN model with subword embedding\n",
    "2. Character based RNN model\n",
    "2. Try different model architecture\n",
    "3. Try different training corprus\n",
    "4. Personalized LM\n",
    "\n",
    "**You are given the following files**:\n",
    "- `Language_Modeling.ipynb`: Notebook file with starter code\n",
    "- `headlines.train`: Training set to train your model\n",
    "- `headlines.dev`: Test set to report your modelâ€™s performance\n",
    "- `glove_300d.csv`: Glove embedding truncated for the vocab in the training data\n",
    "- `../utils/`: folder containing all utility code for the series of homeworks\n",
    "\n",
    "**Deriverables**:\n",
    "- pdf or html of the notebook\n",
    "- A report of your own model if you have "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T19:52:22.037603Z",
     "start_time": "2020-03-15T19:52:21.343054Z"
    }
   },
   "source": [
    "### ======================== Coding starts here ===================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:39:44.068513Z",
     "start_time": "2020-03-22T20:39:33.927889Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os, sys, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# add utils folder to path\n",
    "p = os.path.dirname(os.getcwd())\n",
    "if p not in sys.path:\n",
    "    sys.path = [p] + sys.path\n",
    "\n",
    "from utils.hw4 import (load_data, load_data_char, gen_vocab, START, END, UNK, \n",
    "                       load_embedding)\n",
    "from utils.general import sigmoid, tanh, show_keras_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:39:45.386244Z",
     "start_time": "2020-03-22T20:39:44.072238Z"
    }
   },
   "outputs": [],
   "source": [
    "# The input is trucated for fast iteration\n",
    "# Remember to use the full set of data for your final model training\n",
    "# It may take some time\n",
    "headlines_train = load_data(\"headlines.train\")[:10000]\n",
    "headlines_dev = load_data(\"headlines.dev\")[:100]\n",
    "\n",
    "# Before we begin, let's look at what some of the headlines look like. \n",
    "# Run the following code block as many times as you want to get a sense \n",
    "# of what kind of headlines we hope to generate.\n",
    "for headline in random.sample(headlines_train, 5):\n",
    "    print(START + ' '.join(headline) + END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:39:55.040257Z",
     "start_time": "2020-03-22T20:39:45.406419Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate the vocab list and the embedding \n",
    "# It (might) be helpful to remove low frequency words, so the model learns how to\n",
    "# treat unseen vocabulary\n",
    "vocab, re_vocab = gen_vocab(headlines_train, 4)\n",
    "sent_len = max([len(s) for s in headlines_train]) + 1\n",
    "\n",
    "print(\"Size of vocab: \", len(vocab))\n",
    "print(\"Longest setence length: \", sent_len)\n",
    "\n",
    "# Load the embedding, trick is played to fill the missing vocab\n",
    "# you can look into the source file to see what it actually does\n",
    "# This embedding file is truncated for vocab used in this dataset\n",
    "# If you are to train your own model with your own data, remember to download\n",
    "# the original embedding here: https://nlp.stanford.edu/projects/glove/\n",
    "glove = load_embedding('glove_300d.csv', vocab=vocab)\n",
    "\n",
    "glove.T.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:39:55.155696Z",
     "start_time": "2020-03-22T20:39:55.044811Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transform the DF to np array\n",
    "glove = glove.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:39:55.273785Z",
     "start_time": "2020-03-22T20:39:55.159796Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_label(token):\n",
    "    \"\"\"\n",
    "    Simply transfer a token to its numerical label, if the token is not int\n",
    "    the vocab, return the label of UNK\n",
    "    input: \n",
    "        token: str\n",
    "        \n",
    "    output:\n",
    "        int\n",
    "    \"\"\"\n",
    "    return re_vocab.get(token, re_vocab[UNK])\n",
    "\n",
    "def to_embedding(X):\n",
    "    \"\"\"\n",
    "    For the 2 dimensional input X filled with the vocabulary label, \n",
    "    return an np.array of their embedding\n",
    "    \n",
    "    input:\n",
    "        X: np.array(n_sample, sent_len)\n",
    "        \n",
    "    return:\n",
    "        embdding\n",
    "    \"\"\"\n",
    "    embedding = np.zeros((len(X), len(X[0]), glove.shape[1]))\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        for j in range(len(X[0])):\n",
    "            embedding[i,j,:] = glove[X[i][j]] \n",
    "    \n",
    "    return embedding\n",
    "\n",
    "def sample_with_weight(prob, avoid_UNK=True):\n",
    "    \"\"\"\n",
    "    For a given probability distribution, return a random int sampled by the\n",
    "    probability distribution. \n",
    "    \n",
    "    input:\n",
    "        prob: list of float probability\n",
    "        avoid_UNK: boolean, if UNK should be excluded\n",
    "    \"\"\"\n",
    "    unk_idx = re_vocab[UNK]\n",
    "    \n",
    "    if avoid_UNK: \n",
    "        prob[unk_idx] = 0 # Make sure we do not use UNK in the generated text\n",
    "    \n",
    "    # If the distribution is 0, use uniform distribution\n",
    "    if prob.sum() <= 0:\n",
    "        prob[1:] = 1.0\n",
    "        \n",
    "    return np.random.choice(range(len(prob)), p=prob/prob.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tri-gram (second order) Markov Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build FNN  Bi-Gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:39:56.259783Z",
     "start_time": "2020-03-22T20:39:55.280568Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Reshape\n",
    "\n",
    "n_gram = 2\n",
    "\n",
    "# For simplicity, we use the embedding of words to feed the model, therefore\n",
    "# no need to add a Embedding layer in the begining. But for a possibly better performance\n",
    "# you can add a embedding layer, even better if you use the glove embedding matrix as the\n",
    "# initial value for the embedding layer\n",
    "# This is useful also because we have filled the embedding with random values for those missing\n",
    "# vocabularies, allowing the embedding matrix to relax during training will improve the performance \n",
    "# for these words as well. But be prepared that this would slow down the training\n",
    "FNN_model = Sequential()\n",
    "FNN_model.add(Reshape(target_shape=(n_gram * glove.shape[1],), \n",
    "                      input_shape=(n_gram, glove.shape[1],)))\n",
    "FNN_model.add(Dense(100, activation=\"relu\", name=\"Dense-1\"))\n",
    "FNN_model.add(Dense(len(vocab), activation=\"softmax\", name=\"Dense-2\"))\n",
    "\n",
    "FNN_model.summary()\n",
    "show_keras_model(FNN_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:39:56.387115Z",
     "start_time": "2020-03-22T20:39:56.263841Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "import random\n",
    "\n",
    "def gen_sample_FNN(data, batch_size=1000, one_hot=True):\n",
    "    \"\"\"\n",
    "    For training the model, we need to shift the data by -1 to produce\n",
    "    label, i.g.\n",
    "    [\"word1\", \"word2\", \"word3\", \"word4\"] --> \n",
    "    X: [[START, STSRT],\n",
    "        [START, 1],\n",
    "        [1, 2],\n",
    "        [2, 3],\n",
    "        [3, 4]]\n",
    "    Y: [1, 2, 3, 4, ...] if one_hot is False, the label is translated to \n",
    "        one-hot if ont_hot is True\n",
    "    \n",
    "    inputs:\n",
    "        data: list of list of strings\n",
    "        batch_size: int\n",
    "        one_hot: boolean\n",
    "        \n",
    "    outputs:\n",
    "        X: np.array(batch_size, n_gram, embedding_dim)\n",
    "        Y: np.array(batch_size, ) or np.array(batch_size, vocab_size)\n",
    "        \n",
    "    batch size is used to control the size for each data batch\n",
    "    set batch_size = -1 if you don't want to generate by batch\n",
    "    \"\"\"\n",
    "    if batch_size == -1:\n",
    "        batch_size = sum([len(s) + 1 for s in data])\n",
    "        \n",
    "    while True:\n",
    "        # Use shuffle so the order in each epoch is different\n",
    "        random.shuffle(data)\n",
    "\n",
    "        X, Y = [], []\n",
    "        for d in data:\n",
    "            encodes = [re_vocab[START], re_vocab[START]] +\\\n",
    "                      [to_label(t) for t in d] +\\\n",
    "                      [re_vocab[END]]\n",
    "            for i in range(len(encodes) - 2):\n",
    "                X.append([encodes[i], encodes[i+1]])\n",
    "                Y.append(encodes[i+2])\n",
    "\n",
    "                if len(X) >= batch_size:\n",
    "                    X = to_embedding(X)\n",
    "                    Y = np.array(Y)\n",
    "                    \n",
    "                    if one_hot:\n",
    "                        Y = to_categorical(Y, num_classes=len(re_vocab))\n",
    "                        \n",
    "                    yield X, Y\n",
    "                    X, Y = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:39:56.504701Z",
     "start_time": "2020-03-22T20:39:56.393718Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_text_FNN(model, max_len=sent_len-1, seed=None):\n",
    "    \"\"\"\n",
    "    For a given FNN model, generate text. If seed is not provided,\n",
    "    use START as initial seed.\n",
    "    \n",
    "    inputs:\n",
    "        model: FNN model\n",
    "        max_len: int, maximum length of the setence\n",
    "        seed: str, the seed word used to generate the text\n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    \"\"\"\n",
    "    Add your code here\n",
    "    \n",
    "    hints:\n",
    "    1. It's a trigram model, what your intial seed look like?\n",
    "    2. The prediction of each state should return a list of probability, use the \n",
    "       `sample_with_weight` function to help you sample the next word.\n",
    "    3. When the word END is sampled, you need to stop the setence. Also use the max_len\n",
    "       to force ending the setence to avoid the program running forever.\n",
    "    \"\"\"\n",
    "        \n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:39:56.697802Z",
     "start_time": "2020-03-22T20:39:56.510836Z"
    }
   },
   "outputs": [],
   "source": [
    "# Before we train the model, let first check if the text generation function\n",
    "# works as expected. Don't worry if the sentence doesn't make any sense.\n",
    "# We haven't trained the model yet!\n",
    "generate_text_FNN(FNN_model, seed=\"china\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:39:56.801374Z",
     "start_time": "2020-03-22T20:39:56.700970Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_perplexity_FNN(model, X, y):\n",
    "    \"\"\"\n",
    "    For a given FNN model, and test data, calcualte the perplexity.\n",
    "    The definition of perplexity is:\n",
    "    \n",
    "    perplexity = exp(- \\sum_i log(P_i) / N)\n",
    "    \n",
    "    inputs:\n",
    "        model: FNN model\n",
    "        X: np.array(n_sample, n_gram, embedding_dim)\n",
    "        y: np.array(n_sample), int label of the next word    \n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Add your code here\n",
    "    \n",
    "    hits:\n",
    "        1. First make the prod prediction using the model\n",
    "        2. The probability at the position of y is what you look for\n",
    "    \n",
    "    When you have too much UNK word, you will find the perplexity to be lower, but it doesn't \n",
    "    really mean your model is better, can you think why?\n",
    "    \"\"\"\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:39:56.989407Z",
     "start_time": "2020-03-22T20:39:56.804347Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let check the perplexity for the untrained model\n",
    "# Is your value close to the number of vocabulary? \n",
    "# Is this a coincidence?\n",
    "X_dev_FNN, y_dev_FNN = next(gen_sample_FNN(headlines_dev, batch_size=-1, one_hot=False))\n",
    "\n",
    "calculate_perplexity_FNN(FNN_model, X_dev_FNN, y_dev_FNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T23:12:21.429617Z",
     "start_time": "2020-03-15T23:12:21.302324Z"
    }
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:39:57.098613Z",
     "start_time": "2020-03-22T20:39:56.992714Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's use the function defined above to report the model performance\n",
    "after each epoch\n",
    "\"\"\"\n",
    "def on_epoch_end_FNN(epoch, logs):\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    for i in range(3):\n",
    "        print(generate_text_FNN(FNN_model))\n",
    "    print('Current perplexity on dev data: ', \n",
    "          calculate_perplexity_FNN(FNN_model, X_dev_FNN, y_dev_FNN), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:41:22.859257Z",
     "start_time": "2020-03-22T20:39:57.101814Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import LambdaCallback\n",
    "\n",
    "\"\"\"\n",
    "Notice how the metrics / generated text evolve after each epoch\n",
    "\"\"\"\n",
    "FNN_model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy', 'top_k_categorical_accuracy'])\n",
    "\n",
    "batch_size = 512\n",
    "steps_per_epoch = sum([len(s) + 1 for s in headlines_train]) // batch_size + 1\n",
    "FNN_model.fit_generator(gen_sample_FNN(headlines_train, batch_size=batch_size), \n",
    "                        epochs = 10, steps_per_epoch=steps_per_epoch,\n",
    "                        callbacks=[LambdaCallback(on_epoch_end=on_epoch_end_FNN)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-based RNN Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:41:24.046904Z",
     "start_time": "2020-03-22T20:41:22.861780Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, LSTM, Activation, TimeDistributed\n",
    "\n",
    "# For simplicity, we use the embedding of words to feed the model, therefore\n",
    "# no need to add a Embedding layer in the begining. But for a possibly better performance\n",
    "# you can add a embedding layer, even better if you use the glove embedding matrix as the\n",
    "# initial value for the embedding layer\n",
    "# This is useful also because we have filled the embedding with random values for those missing\n",
    "# vocabularies, allowing the embedding matrix to relax during training will improve the performance \n",
    "# for these words as well. But be prepared that this would slow down the training\n",
    "\n",
    "# Unfortunately Keras does not have an easy way to support dynamic length of input for RNN model.\n",
    "# So we use the sent_len to truncate all the sentences.\n",
    "batch_size = 10\n",
    "RNN_train_model = Sequential()\n",
    "RNN_train_model.add(\n",
    "    LSTM(128, input_shape=(sent_len, glove.shape[1]), return_sequences=True)\n",
    "    )\n",
    "RNN_train_model.add(TimeDistributed(Dense(len(vocab), activation='softmax')))\n",
    "RNN_train_model.summary()\n",
    "show_keras_model(RNN_train_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:41:25.068865Z",
     "start_time": "2020-03-22T20:41:24.051334Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, LSTM, Activation, TimeDistributed\n",
    "\n",
    "\"\"\"\n",
    "# It's tricky to explain why we need the RNN_pred_model. \n",
    "# The RNN_train_model.predict requires a fix length of input (sent_len in our case).\n",
    "# This is not convenient for us because we need to generate the next text one by one.\n",
    "# The trick we play here is to create a shadow model having only 1 time step. We will\n",
    "# copy the parameter of the RNN_train_model to this model once it's trained.\n",
    "# Check generate_text_RNN function to understand details, and there is some discussion \n",
    "# here: \"https://github.com/keras-team/keras/issues/8771\"\n",
    "\"\"\"\n",
    "\n",
    "RNN_pred_model = Sequential()\n",
    "RNN_pred_model.add(\n",
    "    LSTM(128, batch_input_shape=(1, 1, glove.shape[1]), return_sequences=True)\n",
    "    )\n",
    "RNN_pred_model.add(TimeDistributed(Dense(len(vocab), activation='softmax')))\n",
    "RNN_pred_model.summary()\n",
    "show_keras_model(RNN_pred_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:41:25.246426Z",
     "start_time": "2020-03-22T20:41:25.074446Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def gen_sample_RNN(data, batch_size=100, one_hot=True):\n",
    "    \"\"\"\n",
    "    The input is the same to the FNN model, but the output training data is different.\n",
    "    \n",
    "    inputs:\n",
    "        data: list of list of string\n",
    "        batch_size: int\n",
    "        one_hot: boolean\n",
    "        \n",
    "    output:\n",
    "        X: np.array(batch_size, sent_len, embedding_dim)\n",
    "        Y: np.array(batch_size, sent_len, ) or np.array(batch_size, sent_len, vocab_size)\n",
    "    \"\"\"\n",
    "    if batch_size == -1:\n",
    "        batch_size = len(data)\n",
    "        \n",
    "    while True:\n",
    "        # Shuffle the data so data order is different for different epochs\n",
    "        random.shuffle(data)\n",
    "\n",
    "        X, Y = [], []\n",
    "        for s in data:\n",
    "            X.append([to_label(START)] + [to_label(t) for t in s])\n",
    "            Y.append([to_label(t) for t in s] + [to_label(END)])\n",
    "            \n",
    "            if len(X) >= batch_size:   \n",
    "                X = pad_sequences(sequences=X, maxlen=sent_len, padding='post', value=to_label(END))\n",
    "                Y = pad_sequences(sequences=Y, maxlen=sent_len, padding='post', value=to_label(END))\n",
    "          \n",
    "                if one_hot: Y = to_categorical(Y, num_classes=len(re_vocab))\n",
    "                \n",
    "                yield to_embedding(X), Y\n",
    "                \n",
    "                X, Y = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:41:25.397070Z",
     "start_time": "2020-03-22T20:41:25.257325Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_text_RNN(model, max_len=sent_len-1, seed=None):\n",
    "    \"\"\"\n",
    "    Use the RNN_pred_model to generate text. Notice how we use the stateful model to generate\n",
    "    the next word one by one. Make sure you fully understand each line of this code. \n",
    "    \"\"\"\n",
    "    if seed is None:\n",
    "        seed = START\n",
    "        result = []\n",
    "    else:\n",
    "        result = [seed]\n",
    "    \n",
    "    model.reset_states()\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        X = to_embedding([[to_label(seed)]])\n",
    "        idx = sample_with_weight(model.predict(X)[0][0])\n",
    "        \n",
    "        if vocab[idx] == END: break\n",
    "            \n",
    "        seed = vocab[idx]\n",
    "        result.append(seed)\n",
    "        \n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:41:25.670440Z",
     "start_time": "2020-03-22T20:41:25.400064Z"
    }
   },
   "outputs": [],
   "source": [
    "generate_text_RNN(RNN_pred_model, seed=\"china\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T00:48:16.334866Z",
     "start_time": "2020-03-22T00:48:16.220610Z"
    }
   },
   "source": [
    "## Calculate Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:41:25.835637Z",
     "start_time": "2020-03-22T20:41:25.676315Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_perplexity_RNN(model, X, y):\n",
    "    \"\"\"\n",
    "    For a given FNN model, and test data, calcualte the perplexity.\n",
    "    The definition of perplexity is:\n",
    "    \n",
    "    perplexity = exp(- \\sum_i log(P_i) / N)\n",
    "    \n",
    "    inputs:\n",
    "        model: FNN model\n",
    "        X: np.array(n_sample, sent_len, embedding_dim)\n",
    "        y: np.array(n_sample, sent_len), int labels\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Add your code here\n",
    "    \n",
    "    hits:\n",
    "        1. First make the prod prediction using the RNN_train_model\n",
    "        2. The probability at the position of y is what you look for\n",
    "        3. All sentences have fixed length, meaning a sentence can have multiple padding END at the end\n",
    "           of a sentence. Consider stop counting the perplexity once you hit the first END, otherwise\n",
    "           your perplexity will seem too good.\n",
    "    \n",
    "    When you have too much UNK word, you will find the perplexity to be lower, but it doesn't \n",
    "    really mean your model is better, can you think why?\n",
    "    \"\"\"\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:41:26.330668Z",
     "start_time": "2020-03-22T20:41:25.849151Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let check the perplexity for the untrained model\n",
    "# Is your value close to the number of vocabulary? \n",
    "# Is this a coincidence?\n",
    "X_dev_RNN, y_dev_RNN = next(gen_sample_RNN(headlines_dev, batch_size=-1, one_hot=False))\n",
    "\n",
    "calculate_perplexity_RNN(RNN_train_model, X_dev_RNN, y_dev_RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:41:26.481542Z",
     "start_time": "2020-03-22T20:41:26.334649Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's use the function defined above to report the model performance\n",
    "after each epoch\n",
    "\"\"\"\n",
    "def on_epoch_end_RNN(epoch, logs):\n",
    "    RNN_pred_model.set_weights(RNN_train_model.get_weights())\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    for i in range(3):\n",
    "        print(generate_text_RNN(RNN_pred_model))\n",
    "    print('Current perplexity on dev data: ', \n",
    "          calculate_perplexity_RNN(RNN_train_model, X_dev_RNN, y_dev_RNN), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T20:43:25.495611Z",
     "start_time": "2020-03-22T20:42:34.521146Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Notice how the metrics / generated text evolve after each epoch\n",
    "\"\"\"\n",
    "batch_size = 10\n",
    "num_batches = len(headlines_train) // batch_size \n",
    "RNN_train_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "RNN_train_model.fit_generator(gen_sample_RNN(headlines_train, batch_size), num_batches, 3,\n",
    "          callbacks=[LambdaCallback(on_epoch_end=on_epoch_end_RNN)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
